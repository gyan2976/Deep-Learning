{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNTF/HQWkc+vbNFsM751b/W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**https://www.youtube.com/watch?v=g6kQl_EFn84&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=7**"],"metadata":{"id":"srlyjp_6mUx9"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"LYRE53gxmH0u","executionInfo":{"status":"ok","timestamp":1665468887377,"user_tz":-330,"elapsed":25,"user":{"displayName":"Gyan Kumar","userId":"04208958827417762560"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn   # All neural network modules nn.Linear, nn.conv2d, BatchNorm, Loss functions\n","import torch.optim as optim  # For all optimizations algorithms SGD, Adams etc.\n","import torch.nn.functional as F # All functions that don't have any parameters\n","from torch.utils.data import DataLoader # Gives easier dataset management and creates mini batches\n","import torchvision.datasets as datasets  # Has standard dataset we can import in a nice way\n","import torchvision.transforms as transforms  # Transformations we can perform on our dataset"]},{"cell_type":"markdown","source":["**Saving Checkpoint and Model**"],"metadata":{"id":"hBqDURhprHyt"}},{"cell_type":"code","source":["# Create CNN Module\n","class CNN(nn.Module):\n","  def __init__(self, in_channels=1, num_classes=10):\n","    super(CNN, self).__init__()\n","    self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n","    self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n","    self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n","    self.fc1 = nn.Linear(16*7*7, num_classes)\n","\n","  def forward(self, x):\n","    x = F.relu(self.conv1(x))\n","    x = self.pool(x)\n","    x = F.relu(self.conv2(x))\n","    x = self.pool(x)\n","    x = x.reshape(x.shape[0], -1)\n","    x = self.fc1(x)\n","    return x\n","\n","def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n","  print(\"=> Saving checkpoint\")\n","  torch.save(state, filename)\n","\n","\"\"\"\n","def load_checkpoint(checkpoint):\n","  print(\"=> Loading checkpoint\")\n","  model.load_state_dict(checkpoint['state_dict'])\n","  optimizer.load_state_dict(checkpoint['state_dict'])\n","\"\"\"\n","\n","# Set Device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Hyperparameters\n","input_size = 784\n","num_classes = 10\n","learning_rate = 0.001\n","batch_size = 64\n","num_epochs = 5\n","#load_model = True\n","\n","# Load Data\n","train_dataset = datasets.MNIST('/content/', train=True, transform=transforms.ToTensor(), download=True)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_dataset = datasets.MNIST('/content/', train=False, transform=transforms.ToTensor(), download=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Initialize Network\n","model = CNN().to(device)\n","\n","# Loss and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","#load_model(torch.load('/content/my_checkpoint.pth.tar'))\n","\n","# Train Network\n","for epoch in range(num_epochs):\n","  losses = []\n","  \n","  if epoch == 2:\n","    checkpoint = {'state dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n","    save_checkpoint(checkpoint)\n","\n","  for batch_idx,(data, targets) in enumerate(train_loader):\n","    # Get data to cuda if possible\n","    data = data.to(device=device)\n","    targets = targets.to(device=device)\n","\n","    # Forward\n","    scores = model(data)\n","    loss = criterion(scores, targets)\n","    losses.append(loss.item())\n","\n","    # Backward\n","    optimizer.zero_grad()\n","    loss.backward()\n","\n","    # Gradient Descent or Adam Step\n","    optimizer.step()\n","  \n","  mean_loss = sum(losses) / len(losses)\n","  print(f'Loss at epoch {epoch} is {mean_loss:.5f}')\n","\n","# Check accuracy on training & test to see how good our model\n","def check_accuracy(loader, model):\n","  if loader.dataset.train:\n","    print(\"Checking accuracy on training data\")\n","  else:\n","    print(\"Checking accuracy on test data\")\n","  \n","  num_correct = 0\n","  num_samples = 0\n","  model.eval()\n","\n","  with torch.no_grad():\n","    for x, y in loader:\n","      x = x.to(device=device)\n","      y = y.to(device=device)\n","\n","      scores = model(x)\n","      _, predictions = scores.max(1)\n","      num_correct += (predictions == y).sum()\n","      num_samples += predictions.size(0)\n","\n","    #print(f'Got (num_correct)/(num_samples) with accuracy (float(num_correct)/float(num_samples)*100:.2f)')\n","    print(f'Got {num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n","\n","  model.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wT53XoDdmfl3","executionInfo":{"status":"ok","timestamp":1665468928747,"user_tz":-330,"elapsed":41389,"user":{"displayName":"Gyan Kumar","userId":"04208958827417762560"}},"outputId":"498c1686-7101-4b59-c1ea-da61c840506b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Loss at epoch 0 is 0.31987\n","Loss at epoch 1 is 0.10077\n","=> Saving checkpoint\n","Loss at epoch 2 is 0.07266\n","Loss at epoch 3 is 0.05895\n","Loss at epoch 4 is 0.05106\n"]}]},{"cell_type":"code","source":["check_accuracy(train_loader, model)\n","check_accuracy(test_loader, model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mV6ixYMsmVVv","executionInfo":{"status":"ok","timestamp":1665468934014,"user_tz":-330,"elapsed":5311,"user":{"displayName":"Gyan Kumar","userId":"04208958827417762560"}},"outputId":"de128977-f10f-47e5-a242-0a585808ed0e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Checking accuracy on training data\n","Got 59197/60000 with accuracy 98.66\n","Checking accuracy on test data\n","Got 9838/10000 with accuracy 98.38\n"]}]},{"cell_type":"markdown","source":["**Load Checkpoint and Model**"],"metadata":{"id":"7a89LdFprN_b"}},{"cell_type":"code","source":["# Create CNN Module\n","class CNN(nn.Module):\n","  def __init__(self, in_channels=1, num_classes=10):\n","    super(CNN, self).__init__()\n","    self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n","    self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n","    self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n","    self.fc1 = nn.Linear(16*7*7, num_classes)\n","\n","  def forward(self, x):\n","    x = F.relu(self.conv1(x))\n","    x = self.pool(x)\n","    x = F.relu(self.conv2(x))\n","    x = self.pool(x)\n","    x = x.reshape(x.shape[0], -1)\n","    x = self.fc1(x)\n","    return x\n","\n","def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n","  print(\"=> Saving checkpoint\")\n","  torch.save(state, filename)\n","\n","def load_checkpoint(checkpoint):\n","  print(\"=> Loading checkpoint\")\n","  model.load_state_dict(checkpoint['state_dict'])\n","  optimizer.load_state_dict(checkpoint['state_dict'])\n","\n","# Set Device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Hyperparameters\n","input_size = 784\n","num_classes = 10\n","learning_rate = 0.001\n","batch_size = 64\n","num_epochs = 5\n","load_model = True\n","\n","# Load Data\n","train_dataset = datasets.MNIST('/content/', train=True, transform=transforms.ToTensor(), download=True)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_dataset = datasets.MNIST('/content/', train=False, transform=transforms.ToTensor(), download=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Initialize Network\n","model = CNN().to(device)\n","\n","# Loss and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","if load_model:\n","  load_checkpoint(load_model(torch.load('/content/my_checkpoint.pth.tar')))\n","\n","# Train Network\n","for epoch in range(num_epochs):\n","  losses = []\n","  \n","  if epoch % 3 == 0:\n","    checkpoint = {'state dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n","    save_checkpoint(checkpoint)\n","\n","  for batch_idx,(data, targets) in enumerate(train_loader):\n","    # Get data to cuda if possible\n","    data = data.to(device=device)\n","    targets = targets.to(device=device)\n","\n","    # Forward\n","    scores = model(data)\n","    loss = criterion(scores, targets)\n","    losses.append(loss.item())\n","\n","    # Backward\n","    optimizer.zero_grad()\n","    loss.backward()\n","\n","    # Gradient Descent or Adam Step\n","    optimizer.step()\n","  \n","  mean_loss = sum(losses) / len(losses)\n","  print(f'Loss at epoch {epoch} is {mean_loss:.5f}')\n","\n","# Check accuracy on training & test to see how good our model\n","def check_accuracy(loader, model):\n","  if loader.dataset.train:\n","    print(\"Checking accuracy on training data\")\n","  else:\n","    print(\"Checking accuracy on test data\")\n","  \n","  num_correct = 0\n","  num_samples = 0\n","  model.eval()\n","\n","  with torch.no_grad():\n","    for x, y in loader:\n","      x = x.to(device=device)\n","      y = y.to(device=device)\n","\n","      scores = model(x)\n","      _, predictions = scores.max(1)\n","      num_correct += (predictions == y).sum()\n","      num_samples += predictions.size(0)\n","\n","    #print(f'Got (num_correct)/(num_samples) with accuracy (float(num_correct)/float(num_samples)*100:.2f)')\n","    print(f'Got {num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n","\n","  model.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":236},"id":"2xRELSR8rM63","executionInfo":{"status":"error","timestamp":1665469186495,"user_tz":-330,"elapsed":26,"user":{"displayName":"Gyan Kumar","userId":"04208958827417762560"}},"outputId":"8834675f-7771-4fad-870a-77d47bd6f90d"},"execution_count":6,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-ef403d9ec144>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/my_checkpoint.pth.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Train Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'bool' object is not callable"]}]},{"cell_type":"code","source":["check_accuracy(train_loader, model)\n","check_accuracy(test_loader, model)"],"metadata":{"id":"2uSF81ggrNc4","executionInfo":{"status":"aborted","timestamp":1665468935140,"user_tz":-330,"elapsed":26,"user":{"displayName":"Gyan Kumar","userId":"04208958827417762560"}}},"execution_count":null,"outputs":[]}]}