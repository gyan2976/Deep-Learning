{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "Y3BmnU3Ijf79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_diabetes, load_iris, fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, explained_variance_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
        "import torch"
      ],
      "metadata": {
        "id": "g9AcVrV5rPsw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is Autograd in PyTorch?**"
      ],
      "metadata": {
        "id": "NTX_D0komB8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autograd is a core feature of PyTorch that enables automatic differentiation for all operations on tensors. This is particularly useful for optimizing models in machine learning, especially in the training phase where gradients are needed for backpropagation. Here's an overview of how Autograd works:\n",
        "\n",
        "**Key Features of Autograd:**\n",
        "  - **Automatic Gradient Computation:** Autograd automatically calculates gradients of tensors with respect to some scalar value (usually a loss value). This is done through a directed acyclic graph (DAG) where nodes represent tensors and edges represent functions that produce output tensors.\n",
        "\n",
        "  - **Dynamic Computation Graph:** Unlike static computation graphs (as seen in some other frameworks), PyTorch uses a dynamic computation graph. This means that the graph is built on-the-fly during execution, allowing for greater flexibility. You can change the way your network behaves on the fly, making it easier to debug and experiment.\n",
        "\n",
        "  - **Backward Pass:** Once the forward pass is complete and you have computed a loss, you can call the .backward() method on the loss tensor. This triggers the computation of gradients for all tensors that have requires_grad=True, allowing you to perform gradient descent or other optimization techniques.\n",
        "\n",
        "  - **Tracking History:** Autograd tracks all operations performed on tensors that have requires_grad set to True, allowing it to compute gradients using the chain rule during the backward pass.\n",
        "\n",
        "  - **Custom Gradients:** While Autograd handles most gradient computations automatically, it also allows users to define custom gradients for operations when needed."
      ],
      "metadata": {
        "id": "iIkFBNXRmm2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor with requires_grad set to True\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x ** 2 + 3 * x + 1  # Define a function of x\n",
        "\n",
        "# Perform backpropagation to compute the gradient\n",
        "y.backward()\n",
        "\n",
        "# Access the gradient\n",
        "print(x.grad)  # Outputs: tensor(7.)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajHyJ4rwjChB",
        "outputId": "0b8c45f2-713b-452f-844a-ff85d07df503"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(7.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. How does automatic differentiation work in PyTorch?**"
      ],
      "metadata": {
        "id": "JxxWktXknWit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Automatic differentiation in PyTorch is a crucial feature that allows the framework to compute gradients automatically, facilitating the training of neural networks through backpropagation rather than manually calculating derivatives.\n",
        "\n"
      ],
      "metadata": {
        "id": "4pWvo1V3nTXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a tensor with requires_grad=True\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# Define a function\n",
        "y = x ** 2 + 3 * x + 1\n",
        "\n",
        "# Perform backward pass to compute gradients\n",
        "y.backward()\n",
        "\n",
        "# Access the gradient\n",
        "print(f\"The gradient of y with respect to x is: {x.grad.item()}\")  # Outputs: 7.0\n",
        "\n",
        "# Zero out the gradients before the next iteration\n",
        "x.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WovBB3kqnUDI",
        "outputId": "00cc6855-7ff3-4174-8f0e-a767f82e14b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The gradient of y with respect to x is: 7.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. How do you enable gradient tracking for a tensor?**"
      ],
      "metadata": {
        "id": "DCYgLLsboI-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directly When Creating the Tensor\n",
        "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPQXd2prnbly",
        "outputId": "7743e665-b557-4278-a13a-3ec6164b838e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 3.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using .requires_grad_() Method on an Existing Tensor\n",
        "x = torch.tensor([2.0, 3.0])  # Gradient tracking is off by default\n",
        "x.requires_grad_()  # Enables gradient tracking"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luKsgKH9oVgn",
        "outputId": "9f094e8d-b730-462e-ea04-c8c30eb1c0ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 3.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using .detach() to Create a New Tensor with Gradient Tracking Enabled\n",
        "y = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "z = y.detach().requires_grad_(True)\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSv8cEhvoafI",
        "outputId": "446ab65c-d2be-4c35-f491-e9f464012a67"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting a Tensor to requires_grad=True Temporarily with torch.enable_grad\n",
        "with torch.enable_grad():\n",
        "    x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwUgNkmmofgx",
        "outputId": "b7049f24-816f-4b63-8392-3a82c5ce92ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 3.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What does .backward() do in PyTorch?**"
      ],
      "metadata": {
        "id": "gPWBJutMpTlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The .backward() function in PyTorch is a key part of the backpropagation process. When called on a tensor, it computes the gradients of the tensor with respect to some scalar output, typically a loss, and stores these gradients in the .grad attribute of each tensor with requires_grad=True."
      ],
      "metadata": {
        "id": "g-2T4TJlpglq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a tensor with requires_grad=True\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# Define a function of x\n",
        "y = x ** 2 + 3 * x + 1  # y is now a function of x\n",
        "\n",
        "# Compute gradients\n",
        "y.backward()  # This computes dy/dx and stores it in x.grad\n",
        "\n",
        "# Access the gradient\n",
        "print(x.grad)  # Outputs the gradient of y with respect to x, which is 7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxffD4i_omdA",
        "outputId": "6751ba1a-fb54-4c1b-f975-ffb29a1188c6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(7.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. How do you access the gradients of a tensor?**"
      ],
      "metadata": {
        "id": "o22A_M9npnou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- .grad will be None if .backward() hasn’t been called yet or if requires_grad=False.\n",
        "- Gradients accumulate by default, so it’s common to zero them out at the beginning of each training step with optimizer.zero_grad() or x.grad.zero_() to prevent gradient accumulation."
      ],
      "metadata": {
        "id": "Gbi3rMmHqEPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a tensor with requires_grad=True to track gradients\n",
        "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "\n",
        "# Define a function of x\n",
        "y = x ** 2 + 3 * x + 1  # Some computation\n",
        "\n",
        "# Compute gradients\n",
        "y.sum().backward()  # Sum to make it scalar, then backpropagate\n",
        "\n",
        "# Access and print the gradients\n",
        "print(x.grad)  # Outputs the gradient of y with respect to x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ6ZKrK4pkMy",
        "outputId": "beb1b382-91b9-4cd3-8515-3ab93fbe998a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([7., 9.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. How can you prevent a tensor from calculating gradients?**"
      ],
      "metadata": {
        "id": "a6hgOGdBqQ3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using torch.no_grad()\n",
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "with torch.no_grad():\n",
        "    y = x * 2  # No gradients will be calculated for y\n",
        "print(y.requires_grad)  # Outputs: False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TWnU6_cqJ1s",
        "outputId": "45d879df-b045-4d39-b985-529f20d17568"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using .detach()\n",
        "z = x.detach()  # Creates a new tensor without gradient tracking\n",
        "print(z.requires_grad)  # Outputs: False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-_0DBhVqj0T",
        "outputId": "f1131b1b-158c-4513-d091-c8f6d2c40ee5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting requires_grad=False Directly\n",
        "x.requires_grad = False  # Directly disable gradient tracking for x\n",
        "print(x.requires_grad)  # Outputs: False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTCBchEJqoQr",
        "outputId": "05be5934-4f1a-43ee-9ffd-bcadaf31217f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Disabling Gradient Tracking Globally (Less Common)\n",
        "torch.set_grad_enabled(False)\n",
        "# All subsequent operations will not track gradients until turned back on\n",
        "\n",
        "# Enable it again\n",
        "torch.set_grad_enabled(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvSbJf9OqsUz",
        "outputId": "b72ad1e8-38d4-4247-ee07-b8eed5cd31dc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7e06703f8790>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. What is the purpose of torch.no_grad()?**"
      ],
      "metadata": {
        "id": "hNNYmFdargsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The purpose of torch.no_grad() in PyTorch is to temporarily disable gradient tracking during computations, which can reduce memory usage and improve performance. This is especially useful in scenarios like model inference or evaluation, where gradient calculations are unnecessary since we are not updating the model’s weights.\n",
        "- Avoid using torch.no_grad() during training, where you need gradients to update weights. It should only be used when you want to freeze the model’s parameters for specific computations or when you are only interested in predictions, not training or backpropagation."
      ],
      "metadata": {
        "id": "r5FYdknJrrYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. How can you clear gradients in PyTorch?**"
      ],
      "metadata": {
        "id": "Ta8OmxOrr7Eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Using optimizer.zero_grad()\n",
        "- Using model.zero_grad()\n",
        "- Setting Gradients Manually to Zero (Advanced)\n",
        "\n",
        "- Gradients accumulate by default in PyTorch, meaning they add up after each .backward() call. To ensure that each batch updates parameters based only on the current batch, we clear the gradients at the start of each iteration."
      ],
      "metadata": {
        "id": "w88BWASBsEES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. When should you use .detach()?**"
      ],
      "metadata": {
        "id": "PbdgIzIwsWNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use .detach() when you want to:\n",
        "- Prevent gradients from being tracked for intermediate tensors.\n",
        "- Work with tensor values without influencing the backpropagation.\n",
        "- Freeze layers during transfer learning.\n",
        "- Accumulate metrics or statistics without affecting the training process.\n",
        "- Convert tensors to numpy arrays without tracking gradients."
      ],
      "metadata": {
        "id": "XT4ogkYGsgrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. How do you compute gradients for functions involving multiple variables?**"
      ],
      "metadata": {
        "id": "WRcHEKtQsong"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define the variables with requires_grad=True\n",
        "x = torch.tensor(3.0, requires_grad=True)  # Variable x\n",
        "y = torch.tensor(4.0, requires_grad=True)  # Variable y\n",
        "\n",
        "# Step 2: Define the function\n",
        "f = x**2 + y**2  # f(x, y) = x^2 + y^2\n",
        "\n",
        "# Step 3: Compute the gradients\n",
        "f.backward()  # Perform backpropagation to compute gradients\n",
        "\n",
        "# Step 4: Access the gradients\n",
        "print(f\"Gradient with respect to x: {x.grad}\")  # ∂f/∂x\n",
        "print(f\"Gradient with respect to y: {y.grad}\")  # ∂f/∂y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6orjIf4qyaw",
        "outputId": "7ed6129c-aef5-45c9-bf90-d7afed56d6ad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient with respect to x: 6.0\n",
            "Gradient with respect to y: 8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. What is the difference between requires_grad=True and requires_grad=False in PyTorch tensors?**"
      ],
      "metadata": {
        "id": "JzKnF3nc0EQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**requires_grad=True**\n",
        "  - **Gradient Tracking:** When a tensor has requires_grad=True, PyTorch will track all operations on that tensor. This means that it will construct a computation graph dynamically, allowing you to compute gradients with respect to that tensor.\n",
        "  - **Backward Propagation:** You can call .backward() on a scalar (single-valued) tensor that results from operations involving tensors with requires_grad=True. This will compute the gradients of all tensors involved in the computation leading to that scalar tensor.\n",
        "  - **Use Case:** This is typically used for parameters in models (like weights and biases) that need to be updated during training through optimization algorithms like gradient descent.\n",
        "\n",
        "**requires_grad=False**\n",
        "  - **No Gradient Tracking:** When a tensor has requires_grad=False, PyTorch will not track operations on that tensor. As a result, no computation graph will be constructed, and you cannot compute gradients with respect to that tensor.\n",
        "  - **No Backward Propagation:** If you try to call .backward() on a tensor that does not require gradients, it will raise an error unless the tensor is a scalar.\n",
        "  - **Use Case:** This is often used for input data or any tensor that is not a parameter needing optimization. It helps reduce memory usage and computation time since no gradients will be computed for these tensors."
      ],
      "metadata": {
        "id": "E5Pw4qxC0Ip-"
      }
    }
  ]
}