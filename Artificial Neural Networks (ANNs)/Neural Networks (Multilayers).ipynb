{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Random Numbers**"
      ],
      "metadata": {
        "id": "qS2llqvcxAax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class MultiLayerANN:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate=0.01):\n",
        "        # Initialize parameters\n",
        "        self.learning_rate = learning_rate\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # Initialize weights and biases for each layer\n",
        "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            weight = np.random.randn(layer_sizes[i + 1], layer_sizes[i]) * 0.01\n",
        "            bias = np.zeros((layer_sizes[i + 1], 1))\n",
        "            self.weights.append(weight)\n",
        "            self.biases.append(bias)\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def sigmoid_derivative(self, z):\n",
        "        sig = self.sigmoid(z)\n",
        "        return sig * (1 - sig)\n",
        "\n",
        "    def softmax(self, z):\n",
        "        exp_z = np.exp(z - np.max(z))\n",
        "        return exp_z / exp_z.sum(axis=0, keepdims=True)\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        self.A = [X]\n",
        "        for i in range(len(self.weights) - 1):\n",
        "            Z = np.dot(self.weights[i], self.A[-1]) + self.biases[i]\n",
        "            A = self.sigmoid(Z)\n",
        "            self.A.append(A)\n",
        "\n",
        "        # Output layer\n",
        "        Z_out = np.dot(self.weights[-1], self.A[-1]) + self.biases[-1]\n",
        "        A_out = self.softmax(Z_out)\n",
        "        self.A.append(A_out)\n",
        "\n",
        "        return A_out\n",
        "\n",
        "    def compute_cost(self, Y_hat, Y):\n",
        "        m = Y.shape[1]\n",
        "        cost = -np.sum(Y * np.log(Y_hat + 1e-8)) / m\n",
        "        return np.squeeze(cost)\n",
        "\n",
        "    def backward_propagation(self, X, Y):\n",
        "        m = X.shape[1]\n",
        "\n",
        "        # Output layer gradient\n",
        "        dZ = self.A[-1] - Y  # Output layer\n",
        "        self.dW = [1/m * np.dot(dZ, self.A[-2].T)]\n",
        "        self.db = [1/m * np.sum(dZ, axis=1, keepdims=True)]\n",
        "\n",
        "        # Backpropagate for each hidden layer\n",
        "        for i in range(len(self.weights) - 2, -1, -1):  # Start from second-to-last layer\n",
        "            dA = np.dot(self.weights[i + 1].T, dZ)  # Backpropagate the error\n",
        "            dZ = dA * self.sigmoid_derivative(self.A[i + 1])  # Apply the derivative of the activation function\n",
        "            self.dW.insert(0, 1/m * np.dot(dZ, self.A[i].T))  # Insert gradients at the start of the list\n",
        "            self.db.insert(0, 1/m * np.sum(dZ, axis=1, keepdims=True))\n",
        "\n",
        "        # Update weights and biases\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= self.learning_rate * self.dW[i]\n",
        "            self.biases[i] -= self.learning_rate * self.db[i]\n",
        "\n",
        "    def train(self, X_train, Y_train, X_val, Y_val, X_test, Y_test, epochs=10):\n",
        "        for epoch in range(1, epochs + 1):  # Start epoch from 1 to print like 1, 2, 3,...\n",
        "            Y_hat_train = self.forward_propagation(X_train)\n",
        "            cost = self.compute_cost(Y_hat_train, Y_train)\n",
        "            self.backward_propagation(X_train, Y_train)\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            predictions_train = self.predict(X_train)\n",
        "            accuracy_train = np.mean(predictions_train == np.argmax(Y_train, axis=0)) * 100\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            predictions_val = self.predict(X_val)\n",
        "            accuracy_val = np.mean(predictions_val == np.argmax(Y_val, axis=0)) * 100\n",
        "\n",
        "            # Print the accuracies and cost at each epoch (but not final test accuracy)\n",
        "            print(f\"Epoch {epoch}, Cost: {cost:.2f}, Train Accuracy: {accuracy_train:.2f}%, \"\n",
        "                  f\"Validation Accuracy: {accuracy_val:.2f}%\")\n",
        "\n",
        "        # Final testing accuracy (only once at the end of training)\n",
        "        predictions_test = self.predict(X_test)\n",
        "        accuracy_test = np.mean(predictions_test == np.argmax(Y_test, axis=0)) * 100\n",
        "        print(f\"Test Accuracy: {accuracy_test:.2f}%\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        Y_hat = self.forward_propagation(X)\n",
        "        predictions = np.argmax(Y_hat, axis=0)\n",
        "        return predictions\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Simulate some data (e.g., 2 features, 3 classes, 1000 examples)\n",
        "    np.random.seed(1)\n",
        "    X = np.random.randn(2, 1000)\n",
        "    Y = np.random.randint(0, 3, (1, 1000))\n",
        "    Y_one_hot = np.eye(3)[Y.flatten()].T\n",
        "\n",
        "    # Split the data into training, validation, and testing sets\n",
        "    X_train, X_temp, Y_train, Y_temp = train_test_split(X.T, Y_one_hot.T, test_size=0.4, random_state=42)\n",
        "    X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Initialize and train the network with 3 hidden layers\n",
        "    ann = MultiLayerANN(input_size=2, hidden_sizes=[5, 4, 3], output_size=3, learning_rate=0.1)\n",
        "    ann.train(X_train.T, Y_train.T, X_val.T, Y_val.T, X_test.T, Y_test.T, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8Fsq58WhlM3",
        "outputId": "024ea8eb-13b9-47fe-96d5-8487d7fdee3b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Cost: 1.10, Train Accuracy: 36.00%, Validation Accuracy: 38.00%\n",
            "Epoch 2, Cost: 1.10, Train Accuracy: 36.00%, Validation Accuracy: 38.00%\n",
            "Epoch 3, Cost: 1.10, Train Accuracy: 36.00%, Validation Accuracy: 38.00%\n",
            "Epoch 4, Cost: 1.10, Train Accuracy: 36.00%, Validation Accuracy: 38.00%\n",
            "Epoch 5, Cost: 1.10, Train Accuracy: 36.00%, Validation Accuracy: 38.00%\n",
            "Epoch 6, Cost: 1.10, Train Accuracy: 36.00%, Validation Accuracy: 38.00%\n",
            "Epoch 7, Cost: 1.10, Train Accuracy: 36.00%, Validation Accuracy: 38.00%\n",
            "Epoch 8, Cost: 1.10, Train Accuracy: 36.00%, Validation Accuracy: 38.00%\n",
            "Epoch 9, Cost: 1.10, Train Accuracy: 36.00%, Validation Accuracy: 38.00%\n",
            "Epoch 10, Cost: 1.10, Train Accuracy: 36.00%, Validation Accuracy: 38.00%\n",
            "Test Accuracy: 36.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Make Classification Dataset**"
      ],
      "metadata": {
        "id": "1iFdMS9ww5n6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class MultiLayerNet:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, activation_function, loss_function, reg_lambda=0.0):\n",
        "        self.params = {}\n",
        "        self.num_layers = len(hidden_sizes) + 1  # Number of hidden layers + output layer\n",
        "        self.layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
        "\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            self.params[f'W{i}'] = np.random.randn(self.layer_sizes[i-1], self.layer_sizes[i]) / np.sqrt(self.layer_sizes[i-1])\n",
        "            self.params[f'b{i}'] = np.zeros((1, self.layer_sizes[i]))  # Shape (1, layer_size)\n",
        "\n",
        "        self.activation_function = activation_function\n",
        "        self.loss_function = loss_function\n",
        "        self.reg_lambda = reg_lambda\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        sig = self.sigmoid(x)\n",
        "        return sig * (1 - sig)\n",
        "\n",
        "    def forward(self, X):\n",
        "        layer_output = X\n",
        "        self.layer_inputs = []\n",
        "        self.layer_outputs = [X]\n",
        "\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            W, b = self.params[f'W{i}'], self.params[f'b{i}']\n",
        "            layer_input = np.dot(layer_output, W) + b\n",
        "            self.layer_inputs.append(layer_input)\n",
        "            layer_output = self.sigmoid(layer_input)\n",
        "            self.layer_outputs.append(layer_output)\n",
        "\n",
        "        return layer_output\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        delta = output - y  # delta should have shape (n_samples, 1)\n",
        "        dW = {}\n",
        "        db = {}\n",
        "        delta = delta / X.shape[0]\n",
        "\n",
        "        for i in reversed(range(1, self.num_layers + 1)):\n",
        "            layer_input = self.layer_inputs[i-1]\n",
        "            activation_derivative = self.sigmoid_derivative(layer_input)\n",
        "\n",
        "            dW[f'W{i}'] = np.dot(self.layer_outputs[i-1].T, delta * activation_derivative) + self.reg_lambda * self.params[f'W{i}']\n",
        "            db[f'b{i}'] = np.sum(delta * activation_derivative, axis=0, keepdims=True)  # Use keepdims to maintain shape\n",
        "\n",
        "            delta = np.dot(delta * activation_derivative, self.params[f'W{i}'].T)\n",
        "\n",
        "        return dW, db\n",
        "\n",
        "    def loss(self, X, y, output):\n",
        "        data_loss = self.loss_function(output, y)\n",
        "        reg_loss = 0.0\n",
        "\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            reg_loss += 0.5 * self.reg_lambda * np.sum(self.params[f'W{i}'] ** 2)\n",
        "\n",
        "        total_loss = data_loss + reg_loss\n",
        "        return total_loss\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, num_epochs, learning_rate=0.1):\n",
        "        for epoch in range(1, num_epochs + 1):  # Start from 1 instead of 0\n",
        "            # Forward propagation\n",
        "            output_train = self.forward(X_train)\n",
        "\n",
        "            # Backward propagation\n",
        "            dW, db = self.backward(X_train, y_train, output_train)\n",
        "\n",
        "            # Update parameters\n",
        "            for i in range(1, self.num_layers + 1):\n",
        "                self.params[f'W{i}'] -= learning_rate * dW[f'W{i}']\n",
        "                self.params[f'b{i}'] -= learning_rate * db[f'b{i}']\n",
        "\n",
        "            # Print loss and accuracy for each epoch (no skipping)\n",
        "            loss_train = self.loss(X_train, y_train, output_train)\n",
        "            output_val = self.forward(X_val)\n",
        "            loss_val = self.loss(X_val, y_val, output_val)\n",
        "\n",
        "            # Training accuracy\n",
        "            train_pred = np.round(output_train)\n",
        "            train_accuracy = np.mean(train_pred == y_train)\n",
        "\n",
        "            # Validation accuracy\n",
        "            val_pred = np.round(output_val)\n",
        "            val_accuracy = np.mean(val_pred == y_val)\n",
        "\n",
        "            # Print results for every epoch\n",
        "            # print(f\"Epoch {epoch}, Loss (Train): {loss_train}, Loss (Validation): {loss_val}\")\n",
        "            print(f\"Epoch {epoch}, Train Accuracy: {train_accuracy:.2f}, Validation Accuracy: {val_accuracy:.2f}\")\n",
        "\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        output = self.forward(X)\n",
        "        predicted_classes = np.round(output)\n",
        "        accuracy = np.mean(predicted_classes == y)\n",
        "        return accuracy\n",
        "\n",
        "# Define the binary cross-entropy loss function\n",
        "def binary_crossentropy_loss(output, y):\n",
        "    epsilon = 1e-7  # Prevent log(0) errors\n",
        "    output = np.clip(output, epsilon, 1 - epsilon)\n",
        "    return -np.mean(y * np.log(output) + (1 - y) * np.log(1 - output))\n",
        "\n",
        "# Generate a toy classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "y = y.reshape(-1, 1)  # Reshape y for binary classification\n",
        "\n",
        "# Split the dataset into training, validation, and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# Normalize the input data\n",
        "mean = X_train.mean(axis=0)\n",
        "std = X_train.std(axis=0)\n",
        "X_train = (X_train - mean) / std\n",
        "X_val = (X_val - mean) / std\n",
        "X_test = (X_test - mean) / std\n",
        "\n",
        "# Create a multi-layer neural network with 5 hidden layers\n",
        "net = MultiLayerNet(input_size=10, hidden_sizes=[20, 15, 10, 5, 3], output_size=1,\n",
        "                    activation_function='sigmoid', loss_function=binary_crossentropy_loss, reg_lambda=0.01)\n",
        "\n",
        "# Train the network for 1000 epochs\n",
        "net.train(X_train, y_train, X_val, y_val, num_epochs=10, learning_rate=0.01)\n",
        "\n",
        "# Evaluate the trained network on the test set\n",
        "test_accuracy = net.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "s1lwF6CYV8Xj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4e8494d-4734-41b1-ceca-8fc51794fe94"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Accuracy: 0.51, Validation Accuracy: 0.54\n",
            "Epoch 2, Train Accuracy: 0.51, Validation Accuracy: 0.54\n",
            "Epoch 3, Train Accuracy: 0.51, Validation Accuracy: 0.54\n",
            "Epoch 4, Train Accuracy: 0.51, Validation Accuracy: 0.54\n",
            "Epoch 5, Train Accuracy: 0.51, Validation Accuracy: 0.54\n",
            "Epoch 6, Train Accuracy: 0.51, Validation Accuracy: 0.54\n",
            "Epoch 7, Train Accuracy: 0.51, Validation Accuracy: 0.54\n",
            "Epoch 8, Train Accuracy: 0.51, Validation Accuracy: 0.54\n",
            "Epoch 9, Train Accuracy: 0.51, Validation Accuracy: 0.54\n",
            "Epoch 10, Train Accuracy: 0.51, Validation Accuracy: 0.54\n",
            "Test Accuracy: 0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**MNIST Dataset**"
      ],
      "metadata": {
        "id": "yh2zSutjxH91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class MultiLayerNet:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, activation_function, loss_function, reg_lambda=0.0):\n",
        "        self.params = {}\n",
        "        self.num_layers = len(hidden_sizes) + 1  # Number of hidden layers + output layer\n",
        "        self.layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
        "\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            self.params[f'W{i}'] = np.random.randn(self.layer_sizes[i-1], self.layer_sizes[i]) / np.sqrt(self.layer_sizes[i-1])\n",
        "            self.params[f'b{i}'] = np.zeros((1, self.layer_sizes[i]))  # Shape (1, layer_size)\n",
        "\n",
        "        self.activation_function = activation_function\n",
        "        self.loss_function = loss_function\n",
        "        self.reg_lambda = reg_lambda\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        sig = self.sigmoid(x)\n",
        "        return sig * (1 - sig)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # for numerical stability\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def softmax_derivative(self, x):\n",
        "        # Softmax derivative for cross-entropy loss\n",
        "        s = self.softmax(x)\n",
        "        return s * (1 - s)\n",
        "\n",
        "    def forward(self, X):\n",
        "        layer_output = X\n",
        "        self.layer_inputs = []\n",
        "        self.layer_outputs = [X]\n",
        "\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            W, b = self.params[f'W{i}'], self.params[f'b{i}']\n",
        "            layer_input = np.dot(layer_output, W) + b\n",
        "            self.layer_inputs.append(layer_input)\n",
        "            if i == self.num_layers:  # Apply softmax activation on the output layer\n",
        "                layer_output = self.softmax(layer_input)\n",
        "            else:\n",
        "                layer_output = self.sigmoid(layer_input)\n",
        "            self.layer_outputs.append(layer_output)\n",
        "\n",
        "        return layer_output\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        delta = output - y  # delta should have shape (n_samples, 10)\n",
        "        dW = {}\n",
        "        db = {}\n",
        "        delta = delta / X.shape[0]\n",
        "\n",
        "        for i in reversed(range(1, self.num_layers + 1)):\n",
        "            layer_input = self.layer_inputs[i-1]\n",
        "            activation_derivative = self.softmax_derivative(layer_input) if i == self.num_layers else self.sigmoid_derivative(layer_input)\n",
        "\n",
        "            dW[f'W{i}'] = np.dot(self.layer_outputs[i-1].T, delta * activation_derivative) + self.reg_lambda * self.params[f'W{i}']\n",
        "            db[f'b{i}'] = np.sum(delta * activation_derivative, axis=0, keepdims=True)\n",
        "\n",
        "            delta = np.dot(delta * activation_derivative, self.params[f'W{i}'].T)\n",
        "\n",
        "        return dW, db\n",
        "\n",
        "    def loss(self, X, y, output):\n",
        "        data_loss = self.loss_function(output, y)\n",
        "        reg_loss = 0.0\n",
        "\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            reg_loss += 0.5 * self.reg_lambda * np.sum(self.params[f'W{i}'] ** 2)\n",
        "\n",
        "        total_loss = data_loss + reg_loss\n",
        "        return total_loss\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, num_epochs, learning_rate=0.1):\n",
        "        for epoch in range(1, num_epochs + 1):  # Start from 1 instead of 0\n",
        "            # Forward propagation\n",
        "            output_train = self.forward(X_train)\n",
        "\n",
        "            # Backward propagation\n",
        "            dW, db = self.backward(X_train, y_train, output_train)\n",
        "\n",
        "            # Update parameters\n",
        "            for i in range(1, self.num_layers + 1):\n",
        "                self.params[f'W{i}'] -= learning_rate * dW[f'W{i}']\n",
        "                self.params[f'b{i}'] -= learning_rate * db[f'b{i}']\n",
        "\n",
        "            # Print loss and accuracy for each epoch (no skipping)\n",
        "            loss_train = self.loss(X_train, y_train, output_train)\n",
        "            output_val = self.forward(X_val)\n",
        "            loss_val = self.loss(X_val, y_val, output_val)\n",
        "\n",
        "            # Training accuracy\n",
        "            train_pred = np.argmax(output_train, axis=1)\n",
        "            train_accuracy = np.mean(train_pred == np.argmax(y_train, axis=1))\n",
        "\n",
        "            # Validation accuracy\n",
        "            val_pred = np.argmax(output_val, axis=1)\n",
        "            val_accuracy = np.mean(val_pred == np.argmax(y_val, axis=1))\n",
        "\n",
        "            # Print results for every epoch\n",
        "            print(f\"Epoch {epoch}, Train Accuracy: {train_accuracy:.2f}, Validation Accuracy: {val_accuracy:.2f}\")\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        output = self.forward(X)\n",
        "        predicted_classes = np.argmax(output, axis=1)\n",
        "        accuracy = np.mean(predicted_classes == np.argmax(y, axis=1))\n",
        "        return accuracy\n",
        "\n",
        "# Define the categorical cross-entropy loss function\n",
        "def categorical_crossentropy_loss(output, y):\n",
        "    epsilon = 1e-7  # Prevent log(0) errors\n",
        "    output = np.clip(output, epsilon, 1 - epsilon)\n",
        "    return -np.mean(np.sum(y * np.log(output), axis=1))\n",
        "\n",
        "# Load MNIST dataset from OpenML\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X = mnist.data / 255.0  # Normalize pixel values to [0, 1]\n",
        "y = mnist.target.astype(int)\n",
        "\n",
        "# Convert y to one-hot encoding for multi-class classification\n",
        "y_one_hot = np.zeros((y.size, 10))\n",
        "y_one_hot[np.arange(y.size), y] = 1\n",
        "\n",
        "# Split the dataset into training, validation, and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# Create a multi-layer neural network with 5 hidden layers\n",
        "net = MultiLayerNet(input_size=784, hidden_sizes=[128, 64, 32, 16, 8], output_size=10,\n",
        "                    activation_function='sigmoid', loss_function=categorical_crossentropy_loss, reg_lambda=0.01)\n",
        "\n",
        "# Train the network for 10 epochs\n",
        "net.train(X_train, y_train, X_val, y_val, num_epochs=10, learning_rate=0.01)\n",
        "\n",
        "# Evaluate the trained network on the test set\n",
        "test_accuracy = net.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL1078sayacV",
        "outputId": "7906ac10-f84f-47b9-ab22-725f91085e9a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 2, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 3, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 4, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 5, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 6, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 7, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 8, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 9, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 10, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Test Accuracy: 0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class MultiLayerNet:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, activation_function, loss_function, reg_lambda=0.0):\n",
        "        self.params = {}\n",
        "        self.num_layers = len(hidden_sizes) + 1  # Number of hidden layers + output layer\n",
        "        self.layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
        "\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            self.params[f'W{i}'] = np.random.randn(self.layer_sizes[i-1], self.layer_sizes[i]) / np.sqrt(self.layer_sizes[i-1])\n",
        "            self.params[f'b{i}'] = np.zeros((1, self.layer_sizes[i]))  # Shape (1, layer_size)\n",
        "\n",
        "        self.activation_function = activation_function\n",
        "        self.loss_function = loss_function\n",
        "        self.reg_lambda = reg_lambda\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        sig = self.sigmoid(x)\n",
        "        return sig * (1 - sig)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def softmax_derivative(self, x):\n",
        "        s = self.softmax(x)\n",
        "        return s * (1 - s)  # Simplified derivative for softmax (for numerical stability)\n",
        "\n",
        "    def forward(self, X):\n",
        "        layer_output = X\n",
        "        self.layer_inputs = []\n",
        "        self.layer_outputs = [X]\n",
        "\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            W, b = self.params[f'W{i}'], self.params[f'b{i}']\n",
        "            layer_input = np.dot(layer_output, W) + b\n",
        "            self.layer_inputs.append(layer_input)\n",
        "            if i == self.num_layers:\n",
        "                layer_output = self.softmax(layer_input)  # Use softmax for the output layer\n",
        "            else:\n",
        "                layer_output = self.sigmoid(layer_input)\n",
        "            self.layer_outputs.append(layer_output)\n",
        "\n",
        "        return layer_output\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        delta = output - y  # delta should have shape (n_samples, num_classes)\n",
        "        dW = {}\n",
        "        db = {}\n",
        "        delta = delta / X.shape[0]\n",
        "\n",
        "        for i in reversed(range(1, self.num_layers + 1)):\n",
        "            layer_input = self.layer_inputs[i-1]\n",
        "            if i == self.num_layers:\n",
        "                activation_derivative = self.softmax_derivative(layer_input)\n",
        "            else:\n",
        "                activation_derivative = self.sigmoid_derivative(layer_input)\n",
        "\n",
        "            dW[f'W{i}'] = np.dot(self.layer_outputs[i-1].T, delta * activation_derivative) + self.reg_lambda * self.params[f'W{i}']\n",
        "            db[f'b{i}'] = np.sum(delta * activation_derivative, axis=0, keepdims=True)\n",
        "\n",
        "            delta = np.dot(delta * activation_derivative, self.params[f'W{i}'].T)\n",
        "\n",
        "        return dW, db\n",
        "\n",
        "    def loss(self, X, y, output):\n",
        "        data_loss = self.loss_function(output, y)\n",
        "        reg_loss = 0.0\n",
        "\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            reg_loss += 0.5 * self.reg_lambda * np.sum(self.params[f'W{i}'] ** 2)\n",
        "\n",
        "        total_loss = data_loss + reg_loss\n",
        "        return total_loss\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, num_epochs, learning_rate=0.1):\n",
        "        for epoch in range(1, num_epochs + 1):  # Start from 1 instead of 0\n",
        "            # Forward propagation\n",
        "            output_train = self.forward(X_train)\n",
        "\n",
        "            # Backward propagation\n",
        "            dW, db = self.backward(X_train, y_train, output_train)\n",
        "\n",
        "            # Update parameters\n",
        "            for i in range(1, self.num_layers + 1):\n",
        "                self.params[f'W{i}'] -= learning_rate * dW[f'W{i}']\n",
        "                self.params[f'b{i}'] -= learning_rate * db[f'b{i}']\n",
        "\n",
        "            # Print loss and accuracy for each epoch (no skipping)\n",
        "            loss_train = self.loss(X_train, y_train, output_train)\n",
        "            output_val = self.forward(X_val)\n",
        "            loss_val = self.loss(X_val, y_val, output_val)\n",
        "\n",
        "            # Training accuracy\n",
        "            train_pred = np.argmax(output_train, axis=1)  # Get class with max probability\n",
        "            train_accuracy = np.mean(train_pred == np.argmax(y_train, axis=1))\n",
        "\n",
        "            # Validation accuracy\n",
        "            val_pred = np.argmax(output_val, axis=1)  # Get class with max probability\n",
        "            val_accuracy = np.mean(val_pred == np.argmax(y_val, axis=1))\n",
        "\n",
        "            # Print results for every epoch\n",
        "            print(f\"Epoch {epoch}, Train Accuracy: {train_accuracy:.2f}, Validation Accuracy: {val_accuracy:.2f}\")\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        output = self.forward(X)\n",
        "        predicted_classes = np.argmax(output, axis=1)  # Get class with max probability\n",
        "        accuracy = np.mean(predicted_classes == np.argmax(y, axis=1))\n",
        "        return accuracy\n",
        "\n",
        "# Define the categorical cross-entropy loss function\n",
        "def categorical_crossentropy_loss(output, y):\n",
        "    epsilon = 1e-7  # Prevent log(0) errors\n",
        "    output = np.clip(output, epsilon, 1 - epsilon)\n",
        "    return -np.mean(np.sum(y * np.log(output), axis=1))  # Sum over classes for each example\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "X_train_full = X_train_full.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
        "X_test = X_test.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "# Flatten the images from 28x28 to 784-dimensional vectors\n",
        "X_train_full = X_train_full.reshape(X_train_full.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train_full = np.eye(10)[y_train_full]  # One-hot encoding for 10 classes\n",
        "y_test = np.eye(10)[y_test]  # One-hot encoding for 10 classes\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.1, random_state=42)\n",
        "\n",
        "# Create a multi-layer neural network with 5 hidden layers\n",
        "net = MultiLayerNet(input_size=784, hidden_sizes=[512, 256, 128, 64, 32], output_size=10,\n",
        "                    activation_function='sigmoid', loss_function=categorical_crossentropy_loss, reg_lambda=0.01)\n",
        "\n",
        "# Train the network for 10 epochs\n",
        "net.train(X_train, y_train, X_val, y_val, num_epochs=10, learning_rate=0.01)\n",
        "\n",
        "# Evaluate the trained network on the test set\n",
        "test_accuracy = net.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRM2AFhwy3PA",
        "outputId": "dc4d9a29-727b-4e3c-dfae-bc2cbab7b84c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 2, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 3, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 4, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 5, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 6, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 7, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 8, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 9, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Epoch 10, Train Accuracy: 0.11, Validation Accuracy: 0.11\n",
            "Test Accuracy: 0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class MultiLayerNet:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, activation_function, loss_function, reg_lambda=0.0):\n",
        "        self.params = {}\n",
        "        self.num_layers = len(hidden_sizes) + 1  # Number of hidden layers + output layer\n",
        "        self.layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
        "\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            self.params[f'W{i}'] = np.random.randn(self.layer_sizes[i-1], self.layer_sizes[i]) / np.sqrt(self.layer_sizes[i-1])\n",
        "            self.params[f'b{i}'] = np.zeros((1, self.layer_sizes[i]))  # Shape (1, layer_size)\n",
        "\n",
        "        self.activation_function = activation_function\n",
        "        self.loss_function = loss_function\n",
        "        self.reg_lambda = reg_lambda\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        sig = self.sigmoid(x)\n",
        "        return sig * (1 - sig)\n",
        "\n",
        "    def forward(self, X):\n",
        "        layer_output = X\n",
        "        self.layer_inputs = []\n",
        "        self.layer_outputs = [X]\n",
        "\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            W, b = self.params[f'W{i}'], self.params[f'b{i}']\n",
        "            layer_input = np.dot(layer_output, W) + b\n",
        "            self.layer_inputs.append(layer_input)\n",
        "            layer_output = self.sigmoid(layer_input)\n",
        "            self.layer_outputs.append(layer_output)\n",
        "\n",
        "        return layer_output\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        delta = output - y  # delta should have shape (n_samples, 1)\n",
        "        dW = {}\n",
        "        db = {}\n",
        "        delta = delta / X.shape[0]\n",
        "\n",
        "        for i in reversed(range(1, self.num_layers + 1)):\n",
        "            layer_input = self.layer_inputs[i-1]\n",
        "            activation_derivative = self.sigmoid_derivative(layer_input)\n",
        "\n",
        "            dW[f'W{i}'] = np.dot(self.layer_outputs[i-1].T, delta * activation_derivative) + self.reg_lambda * self.params[f'W{i}']\n",
        "            db[f'b{i}'] = np.sum(delta * activation_derivative, axis=0, keepdims=True)  # Use keepdims to maintain shape\n",
        "\n",
        "            delta = np.dot(delta * activation_derivative, self.params[f'W{i}'].T)\n",
        "\n",
        "        return dW, db\n",
        "\n",
        "    def loss(self, X, y, output):\n",
        "        data_loss = self.loss_function(output, y)\n",
        "        reg_loss = 0.0\n",
        "\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            reg_loss += 0.5 * self.reg_lambda * np.sum(self.params[f'W{i}'] ** 2)\n",
        "\n",
        "        total_loss = data_loss + reg_loss\n",
        "        return total_loss\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, num_epochs, learning_rate=0.1):\n",
        "        for epoch in range(1, num_epochs + 1):  # Start from 1 instead of 0\n",
        "            # Forward propagation\n",
        "            output_train = self.forward(X_train)\n",
        "\n",
        "            # Backward propagation\n",
        "            dW, db = self.backward(X_train, y_train, output_train)\n",
        "\n",
        "            # Update parameters\n",
        "            for i in range(1, self.num_layers + 1):\n",
        "                self.params[f'W{i}'] -= learning_rate * dW[f'W{i}']\n",
        "                self.params[f'b{i}'] -= learning_rate * db[f'b{i}']\n",
        "\n",
        "            # Print loss and accuracy for each epoch (no skipping)\n",
        "            loss_train = self.loss(X_train, y_train, output_train)\n",
        "            output_val = self.forward(X_val)\n",
        "            loss_val = self.loss(X_val, y_val, output_val)\n",
        "\n",
        "            # Training accuracy\n",
        "            train_pred = np.round(output_train)\n",
        "            train_accuracy = np.mean(train_pred == y_train)\n",
        "\n",
        "            # Validation accuracy\n",
        "            val_pred = np.round(output_val)\n",
        "            val_accuracy = np.mean(val_pred == y_val)\n",
        "\n",
        "            # Print results for every epoch\n",
        "            print(f\"Epoch {epoch}, Train Accuracy: {train_accuracy:.2f}, Validation Accuracy: {val_accuracy:.2f}\")\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        output = self.forward(X)\n",
        "        predicted_classes = np.round(output)\n",
        "        accuracy = np.mean(predicted_classes == y)\n",
        "        return accuracy\n",
        "\n",
        "# Define the categorical cross-entropy loss function\n",
        "def categorical_crossentropy_loss(output, y):\n",
        "    epsilon = 1e-7  # Prevent log(0) errors\n",
        "    output = np.clip(output, epsilon, 1 - epsilon)\n",
        "    return -np.mean(np.sum(y * np.log(output), axis=1))\n",
        "\n",
        "# Load the MNIST dataset using TensorFlow\n",
        "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "X_train_full = X_train_full.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
        "X_test = X_test.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "# Flatten the images from 28x28 to 784-dimensional vectors\n",
        "X_train_full = X_train_full.reshape(X_train_full.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train_full = np.eye(10)[y_train_full]  # One-hot encoding for 10 classes\n",
        "y_test = np.eye(10)[y_test]  # One-hot encoding for 10 classes\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.1, random_state=42)\n",
        "\n",
        "# Create a multi-layer neural network with 5 hidden layers\n",
        "net = MultiLayerNet(input_size=784, hidden_sizes=[512, 256, 128, 64, 32], output_size=10,\n",
        "                    activation_function='sigmoid', loss_function=categorical_crossentropy_loss, reg_lambda=0.01)\n",
        "\n",
        "# Train the network for 10 epochs\n",
        "net.train(X_train, y_train, X_val, y_val, num_epochs=10, learning_rate=0.01)\n",
        "\n",
        "# Evaluate the trained network on the test set\n",
        "test_accuracy = net.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSqndRw7y5xj",
        "outputId": "e3b99b3a-2d64-4f96-8fe6-8a8389ecfbfa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Accuracy: 0.58, Validation Accuracy: 0.58\n",
            "Epoch 2, Train Accuracy: 0.58, Validation Accuracy: 0.58\n",
            "Epoch 3, Train Accuracy: 0.58, Validation Accuracy: 0.66\n",
            "Epoch 4, Train Accuracy: 0.66, Validation Accuracy: 0.66\n",
            "Epoch 5, Train Accuracy: 0.66, Validation Accuracy: 0.66\n",
            "Epoch 6, Train Accuracy: 0.66, Validation Accuracy: 0.66\n",
            "Epoch 7, Train Accuracy: 0.66, Validation Accuracy: 0.66\n",
            "Epoch 8, Train Accuracy: 0.66, Validation Accuracy: 0.66\n",
            "Epoch 9, Train Accuracy: 0.66, Validation Accuracy: 0.66\n",
            "Epoch 10, Train Accuracy: 0.66, Validation Accuracy: 0.66\n",
            "Test Accuracy: 0.66\n"
          ]
        }
      ]
    }
  ]
}