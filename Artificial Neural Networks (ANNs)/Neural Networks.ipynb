{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "ezYkvqQCa109"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "-HBTUzIfalA2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code**"
      ],
      "metadata": {
        "id": "Go-3f1-Ra5gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleANN:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights_input_hidden = np.random.randn(hidden_size, input_size) * 0.01\n",
        "        self.bias_hidden = np.zeros((hidden_size, 1))\n",
        "        self.weights_hidden_output = np.random.randn(output_size, hidden_size) * 0.01\n",
        "        self.bias_output = np.zeros((output_size, 1))\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def sigmoid_derivative(self, z):\n",
        "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
        "\n",
        "    def relu(self, z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def relu_derivative(self, z):\n",
        "        return (z > 0).astype(float)\n",
        "\n",
        "    def softmax(self, z):\n",
        "        exp_z = np.exp(z - np.max(z))\n",
        "        return exp_z / exp_z.sum(axis=0, keepdims=True)\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        self.Z1 = np.dot(self.weights_input_hidden, X) + self.bias_hidden\n",
        "        self.A1 = self.relu(self.Z1)\n",
        "        self.Z2 = np.dot(self.weights_hidden_output, self.A1) + self.bias_output\n",
        "        self.A2 = self.softmax(self.Z2)\n",
        "        return self.A2\n",
        "\n",
        "    def compute_cost(self, Y_hat, Y):\n",
        "        m = Y.shape[1]\n",
        "        cost = -np.sum(Y * np.log(Y_hat + 1e-8)) / m\n",
        "        return np.squeeze(cost)\n",
        "\n",
        "    def backward_propagation(self, X, Y):\n",
        "        m = X.shape[1]\n",
        "\n",
        "        dZ2 = self.A2 - Y\n",
        "        dW2 = (1 / m) * np.dot(dZ2, self.A1.T)\n",
        "        db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "        dA1 = np.dot(self.weights_hidden_output.T, dZ2)\n",
        "        dZ1 = dA1 * self.relu_derivative(self.Z1)\n",
        "        dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
        "        db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "        self.weights_hidden_output -= self.learning_rate * dW2\n",
        "        self.bias_output -= self.learning_rate * db2\n",
        "        self.weights_input_hidden -= self.learning_rate * dW1\n",
        "        self.bias_hidden -= self.learning_rate * db1\n",
        "\n",
        "    def accuracy(self, Y_hat, Y):\n",
        "        predictions = np.argmax(Y_hat, axis=0)\n",
        "        targets = np.argmax(Y, axis=0)\n",
        "        return np.mean(predictions == targets) * 100\n",
        "\n",
        "    def train(self, X_train, Y_train, X_val, Y_val, epochs=10):\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            Y_hat_train = self.forward_propagation(X_train)\n",
        "            cost = self.compute_cost(Y_hat_train, Y_train)\n",
        "            self.backward_propagation(X_train, Y_train)\n",
        "\n",
        "            if epoch % 1 == 0:  # Print every epoch\n",
        "                train_accuracy = self.accuracy(Y_hat_train, Y_train)\n",
        "                Y_hat_val = self.forward_propagation(X_val)\n",
        "                val_accuracy = self.accuracy(Y_hat_val, Y_val)\n",
        "                print(f\"Epoch {epoch}, Cost: {cost:.4f}, Train Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        Y_hat = self.forward_propagation(X)\n",
        "        predictions = np.argmax(Y_hat, axis=0)\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "KewKbswsanUW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load MNIST dataset\n",
        "# (X_train_full, Y_train_full), (X_test_full, Y_test_full) = mnist.load_data()\n",
        "# X_train_full = X_train_full.reshape(X_train_full.shape[0], -1).T / 255.0  # Flatten and normalize\n",
        "# X_test_full = X_test_full.reshape(X_test_full.shape[0], -1).T / 255.0\n",
        "# Y_train_one_hot = np.eye(10)[Y_train_full]  # One-hot encode labels\n",
        "# Y_test_one_hot = np.eye(10)[Y_test_full]\n",
        "# X_train, X_val, Y_train, Y_val = train_test_split(X_train_full.T, Y_train_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Numbers Dataset\n",
        "np.random.seed(1)\n",
        "X = np.random.randn(2, 150).T\n",
        "Y = np.random.randint(0, 3, 150)\n",
        "Y_one_hot = np.eye(3)[Y]\n",
        "\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y_one_hot, test_size=0.3, random_state=42)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "X_train, X_val, X_test = X_train.T, X_val.T, X_test.T\n",
        "Y_train, Y_val, Y_test = Y_train.T, Y_val.T, Y_test.T\n",
        "\n",
        "ann = SimpleANN(input_size=2, hidden_size=5, output_size=3, learning_rate=0.1)\n",
        "ann.train(X_train, Y_train, X_val, Y_val, epochs=10)\n",
        "\n",
        "Y_hat_test = ann.forward_propagation(X_test)\n",
        "test_accuracy = ann.accuracy(Y_hat_test, Y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSayOTU0andb",
        "outputId": "4616faab-f125-4d0c-f726-a1bf370b424e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Cost: 1.0986, Train Accuracy: 35.24%, Validation Accuracy: 22.73%\n",
            "Epoch 2, Cost: 1.0983, Train Accuracy: 38.10%, Validation Accuracy: 22.73%\n",
            "Epoch 3, Cost: 1.0979, Train Accuracy: 38.10%, Validation Accuracy: 22.73%\n",
            "Epoch 4, Cost: 1.0977, Train Accuracy: 38.10%, Validation Accuracy: 22.73%\n",
            "Epoch 5, Cost: 1.0974, Train Accuracy: 38.10%, Validation Accuracy: 22.73%\n",
            "Epoch 6, Cost: 1.0971, Train Accuracy: 38.10%, Validation Accuracy: 22.73%\n",
            "Epoch 7, Cost: 1.0969, Train Accuracy: 38.10%, Validation Accuracy: 22.73%\n",
            "Epoch 8, Cost: 1.0967, Train Accuracy: 38.10%, Validation Accuracy: 22.73%\n",
            "Epoch 9, Cost: 1.0964, Train Accuracy: 38.10%, Validation Accuracy: 22.73%\n",
            "Epoch 10, Cost: 1.0962, Train Accuracy: 38.10%, Validation Accuracy: 22.73%\n",
            "Test Accuracy: 21.74%\n"
          ]
        }
      ]
    }
  ]
}